{"cells":[{"cell_type":"code","execution_count":1,"id":"ce84cb80","metadata":{},"outputs":[],"source":["import os\n","os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 pyspark-shell'"]},{"cell_type":"code","execution_count":2,"id":"2f0cbaea","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"]},{"name":"stderr","output_type":"stream","text":["Ivy Default Cache set to: /root/.ivy2/cache\n","The jars for the packages stored in: /root/.ivy2/jars\n","org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",":: resolving dependencies :: org.apache.spark#spark-submit-parent-b4e4886c-b385-40e0-8941-cde23bf18d8d;1.0\n","\tconfs: [default]\n","\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n","\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n","\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n","\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n","\tfound org.lz4#lz4-java;1.7.1 in central\n","\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n","\tfound org.slf4j#slf4j-api;1.7.30 in central\n","\tfound org.spark-project.spark#unused;1.0.0 in central\n","\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",":: resolution report :: resolve 650ms :: artifacts dl 12ms\n","\t:: modules in use:\n","\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n","\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n","\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n","\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n","\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n","\torg.lz4#lz4-java;1.7.1 from central in [default]\n","\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n","\torg.spark-project.spark#unused;1.0.0 from central in [default]\n","\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n","\t---------------------------------------------------------------------\n","\t|                  |            modules            ||   artifacts   |\n","\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n","\t---------------------------------------------------------------------\n","\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n","\t---------------------------------------------------------------------\n",":: retrieving :: org.apache.spark#spark-submit-parent-b4e4886c-b385-40e0-8941-cde23bf18d8d\n","\tconfs: [default]\n","\t0 artifacts copied, 9 already retrieved (0kB/11ms)\n","Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","22/04/04 16:44:33 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n","22/04/04 16:44:33 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n","22/04/04 16:44:33 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n","22/04/04 16:44:33 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n","22/04/04 16:44:36 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.2.jar added multiple times to distributed cache.\n","22/04/04 16:44:36 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.2.jar added multiple times to distributed cache.\n","22/04/04 16:44:36 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar added multiple times to distributed cache.\n","22/04/04 16:44:36 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar added multiple times to distributed cache.\n","22/04/04 16:44:36 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar added multiple times to distributed cache.\n","22/04/04 16:44:36 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar added multiple times to distributed cache.\n","22/04/04 16:44:36 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar added multiple times to distributed cache.\n","22/04/04 16:44:36 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar added multiple times to distributed cache.\n","22/04/04 16:44:36 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar added multiple times to distributed cache.\n"]}],"source":["from pyspark.sql.types import StructType, StringType, FloatType\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import explode\n","from pyspark.sql.functions import split\n","from pyspark.ml.classification import RandomForestClassificationModel\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from pyspark.ml.linalg import Vectors\n","from pyspark.sql import Row\n","import pyspark.sql.functions as f\n","from pyspark.ml import PipelineModel\n","from itertools import chain\n","\n","spark = SparkSession\\\n","    .builder\\\n","    .appName(\"Iris-Prediction\")\\\n","    .config(\"spark.driver.extraClassPath\", \"/home/ubuntu/jars/spark-sql-kafka-0-10_2.12-3.1.2.jar,/home/ubuntu/jars/commons-pool2-2.11.0.jar\")\\\n","    .getOrCreate()\n","\n","spark.sparkContext.setLogLevel('WARN')"]},{"cell_type":"code","execution_count":3,"id":"0bb34b00","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","root\n"," |-- value: string (nullable = true)\n","\n"]}],"source":["df = spark.readStream.format('kafka').option('kafka.bootstrap.servers', '10.188.0.2:9092').option(\"startingOffsets\", \"earliest\").option('subscribe', 'iris-data').option(\"failOnDataLoss\", \"false\").load()\n","df = df.selectExpr(\"CAST(value AS STRING)\")\n","\n","schema = StructType()\\\n","    .add(\"sepal_length\", FloatType())\\\n","    .add(\"sepal_width\", FloatType())\\\n","    .add(\"petal_length\", FloatType())\\\n","    .add(\"petal_width\", FloatType())\\\n","    .add(\"species\", StringType())\n","\n","print(df.isStreaming)\n","\n","df.printSchema()\n","\n","df = df.select(f.from_json(f.decode(df.value, 'utf-8'), schema=schema).alias(\"input\"))\n","df = df.select(\"input.*\")"]},{"cell_type":"code","execution_count":4,"id":"03d0a426","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/04/04 16:44:44 WARN org.apache.spark.sql.streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-e61cb8ee-a68b-490f-b8b2-6e5c31d7beb1. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n","22/04/04 16:44:44 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"]}],"source":["query3 = df.writeStream.outputMode('update').format('console').start()\n"]},{"cell_type":"code","execution_count":5,"id":"660f6232","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/04/04 16:44:45 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \n","java.lang.InterruptedException\n","\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n","\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n","\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n","\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["-------------------------------------------\n","Batch: 0\n","-------------------------------------------\n","+------------+-----------+------------+-----------+-----------+\n","|sepal_length|sepal_width|petal_length|petal_width|    species|\n","+------------+-----------+------------+-----------+-----------+\n","|         4.9|        3.0|         1.4|        0.2|Iris-setosa|\n","|         4.7|        3.2|         1.3|        0.2|Iris-setosa|\n","|         4.6|        3.1|         1.5|        0.2|Iris-setosa|\n","|         5.0|        3.6|         1.4|        0.2|Iris-setosa|\n","|         5.4|        3.9|         1.7|        0.4|Iris-setosa|\n","|         4.6|        3.4|         1.4|        0.3|Iris-setosa|\n","|         5.0|        3.4|         1.5|        0.2|Iris-setosa|\n","|         4.4|        2.9|         1.4|        0.2|Iris-setosa|\n","|         4.9|        3.1|         1.5|        0.1|Iris-setosa|\n","|         5.4|        3.7|         1.5|        0.2|Iris-setosa|\n","|         4.8|        3.4|         1.6|        0.2|Iris-setosa|\n","|         4.8|        3.0|         1.4|        0.1|Iris-setosa|\n","|         4.3|        3.0|         1.1|        0.1|Iris-setosa|\n","|         5.8|        4.0|         1.2|        0.2|Iris-setosa|\n","|         5.7|        4.4|         1.5|        0.4|Iris-setosa|\n","|         5.4|        3.9|         1.3|        0.4|Iris-setosa|\n","|         5.1|        3.5|         1.4|        0.3|Iris-setosa|\n","|         5.7|        3.8|         1.7|        0.3|Iris-setosa|\n","|         5.1|        3.8|         1.5|        0.3|Iris-setosa|\n","|         5.4|        3.4|         1.7|        0.2|Iris-setosa|\n","+------------+-----------+------------+-----------+-----------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Model Loaded....\n"]},{"name":"stderr","output_type":"stream","text":["22/04/04 16:44:59 WARN org.apache.spark.ml.feature.StringIndexerModel: Input column class does not exist during transformation. Skip StringIndexerModel for this column.\n"]}],"source":["model_path = 'gs://big-data-lab-madhurj/Ass7/Pipeline_Model'\n","model = PipelineModel.load(model_path)\n","print('Model Loaded....')\n","\n","predictions = model.transform(df)\n","\n","mapping = dict(zip([0.0,1.0,2.0], ['Iris-setosa','Iris-versicolor','Iris-virginica']))\n","mapping_expr = f.create_map([f.lit(x) for x in chain(*mapping.items())])\n","output_df = predictions.withColumn('prediction', mapping_expr[f.col(\"prediction\")])[['prediction','species']]\n","\n","output_df = output_df.withColumn('correct', f.when((f.col('prediction')=='Iris-setosa') & (f.col('species')=='Iris-setosa'),1).when((f.col('prediction')=='Iris-versicolor') & (f.col('species')=='Iris-versicolor'),1).when((f.col('prediction')=='Iris-virginica') & (f.col('species')=='Iris-virginica'),1).otherwise(0))\n","\n","df_acc = output_df.select(f.format_number(f.avg('correct')*100,2).alias('accuracy'))\n","\n","output_df2 = output_df[['prediction','species','correct']]\n","output_df2.createOrReplaceTempView('output')"]},{"cell_type":"code","execution_count":null,"id":"4418e0ef","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/04/04 16:44:59 WARN org.apache.spark.sql.streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-8745b3c2-deb6-466c-8cb1-3e71326cd594. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n","22/04/04 16:44:59 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n","22/04/04 16:44:59 WARN org.apache.spark.sql.streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-10ef7f80-0603-4e4e-aa26-71dd181266d4. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n","22/04/04 16:44:59 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["-------------------------------------------\n","Batch: 0\n","-------------------------------------------\n","+--------+\n","|accuracy|\n","+--------+\n","|   83.89|\n","+--------+\n","\n","-------------------------------------------\n","Batch: 0\n","-------------------------------------------\n","+-----------+-----------+-------+\n","| prediction|    species|correct|\n","+-----------+-----------+-------+\n","|Iris-setosa|Iris-setosa|      1|\n","|Iris-setosa|Iris-setosa|      1|\n","|Iris-setosa|Iris-setosa|      1|\n","|Iris-setosa|Iris-setosa|      1|\n","|Iris-setosa|Iris-setosa|      1|\n","|Iris-setosa|Iris-setosa|      1|\n","|Iris-setosa|Iris-setosa|      1|\n","|Iris-setosa|Iris-setosa|      1|\n","|Iris-setosa|Iris-setosa|      1|\n","|Iris-setosa|Iris-setosa|      1|\n","|Iris-setosa|Iris-setosa|      1|\n","|Iris-setosa|Iris-setosa|      1|\n","|Iris-setosa|Iris-setosa|      1|\n","|Iris-setosa|Iris-setosa|      1|\n","|Iris-setosa|Iris-setosa|      1|\n","|Iris-setosa|Iris-setosa|      1|\n","|Iris-setosa|Iris-setosa|      1|\n","|Iris-setosa|Iris-setosa|      1|\n","|Iris-setosa|Iris-setosa|      1|\n","|Iris-setosa|Iris-setosa|      1|\n","+-----------+-----------+-------+\n","only showing top 20 rows\n","\n"]}],"source":["query1 = output_df2.writeStream.queryName(\"output\").outputMode('update').format('console').start()\n","query2 = df_acc.writeStream.outputMode('update').format('console').start()\n","\n","query1.awaitTermination()\n","query2.awaitTermination()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":5}